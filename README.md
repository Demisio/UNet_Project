# U-Net
The general architecture of the project is taken from:
https://git.ee.ethz.ch/baumgach/discriminative_learning_toolbox and modified where needed.
The modified Unet itself was written by Dr. Christine Tanner (CVL, ETH).

Note: The code was executed on a cluster via an interactive terminal shell in the code directory. 
All paths are relative. If submission of a batch file is desired, some paths might have to be set to absolute ones.

A short overview of the most relevant scripts is provided at the bottom of the README.
## Installation
1.  Clone or download the repository:
    <pre> git clone https://github.com/Demisio/UNet.git </pre>
    
2.  Create an environment for the packages to be installed. With conda, e.g: 
    <pre>conda create -n UDCT python=3.6</pre>

2.  Install requirements via: 
    <pre> pip install -r requirements.txt </pre>
    
4.  Required CUDA / CUDNN versions:
    <pre>Python 3.6, TF 1.4: CUDA 8.0 / CUDNN 6.0</pre>
    
## Creating a Dataset, Data Splits and Log Folders
1.  Images are given in ./Data/Heart/3D. Creating a hdf5 dataset is achieved by navigating into the folder dataprocessing
    and executing the file heart_augment_loader.py
    
    The parameters 'syn_data_path' and 'raw_data_path' data path have to be modified to correspond
    to the desired ground truth and raw data respectively. 
    Also, choose a desired filename for the h5 file, e.g::
    
    <pre>cd data_processing
    
    filename = 'aug_heart_data.h5'
    raw_data_path = './../Data/Heart/3D/Raw/'
    syn_data_path = './../Data/Heart/3D/Segmented_og_labels/'
    
    python heart_augment_loader.py</pre>
    
2.  Creating the test set can be done by executing:
    <pre>python heart_test_loader.py</pre>
    
2.  Datasplits are already provided but can be generated by navigating into the folder ./train_test_split and executing
    the file create_splits.py
    <pre>cd train_test_split
    python create_splits.py</pre>
    
3.  A dictionary, called "logs" has to be created in the main Project folder (if not present). The dictionary should
    contain subfolders with the name of the experiment and 5 subsubfolders called fold1 until fold5.
    For the current project e.g.:
    <pre>./logs/Heart_full/fold3</pre>
    
    (The folder should also be automatically created, if not provided)
    
4.  For creating a dataset with limited data, run heart_augment_loader.py once more but change parameters
    in the file as follows:
    <pre>
    filename = 'aug_heart_data_very_limited.h5'
    raw_data_path = './../Data/Heart/3D/Raw_lim_data/'
    syn_data_path = './../Data/Heart/3D/Segmented_og_labels_lim_data/'
    
    python heart_augment_loader.py</pre>
    
5.  For creating the dataset with GAN-augmented data, look into the README of the UDCT
    (Last point in section "Creating a Dataset with Limited Data "). A hdf5 file has to be created there and copied into the subfolder 'data_processing'
    of this model here. 

## Changing Model configurations and Training the Model

1.  All relevant model parameters are defined in the file:
    <pre>./segmenter/experiments/heart_config.py</pre>
2.  Training a Model with different amounts of data requires changing the variable 'mode' as follows:
    <pre>mode = 'full'    #all standard augmented data
    mode = 'limited' #limited, standard augmented data
    mode = 'GAN'     #GAN augmented data</pre>
    
2.  Execute Training via:
    <pre>python segmenter_train.py</pre>
    
## Testing the Model
1.  After training (or when using the pretrained models), change the variable 'set' in heart_config.py depending on your needs:

    <pre>set = 'test' #use test set at test time
    set = 'train' #use training set with pre-trained model
    set = 'validation' #use validation set with pre-trained model</pre>
    
2.  Ensure that you have a dictionary in the folder 'Results' to save the relevant metrics, e.g.:
    <pre>./Results/Heart
    ./Results/Heart_limited</pre>
    
3.  Run the test as follows:
    <pre>python segmenter_test.py</pre>
    
4.  navigate into the folder 'Plotting':
    <pre>cd Results/Plotting</pre>
    
    and execute:
    <pre>python metric_plots.py</pre>
    
    make sure to set the variable 'bar_dir' inside the file to the folder containing the relevant pickle file, e.g.:
    <pre>bar_dir = './../Heart_all/'
    sample_nr = 0 #define sample volume for which results should be plotted (e.g. 0 or 1 for  test set)</pre>
    
### Relevant Files

<b>segmenter/experiments/heart_config.py</b><br />
Contains all relevant model parameters, change here to conduct different experiments
<br /> 

<b>segmenter/model_segmenter.py</b><br />
Contains the overall model flow and pipeline, interacts with most of the other files presented here
<br />

<b>segmenter_train.py</b><br />
Execute for training the model, takes parameters from heart_config.py
<br />

<b>segmenter_test.py</b><br />
Test a trained model, create pickle dictionaries containing results
<br />

<b>data_processing/heart_augment_loader.py</b><br />
Creates hdf5 data based on the volumes, necessary for running the model
<br />

<b>data_processing/heart_test_loader.py</b><br />
Creates test set hdf5 data (non-augmented, symmetric crops)
<br />

<b>models/networks.py</b><br />
Contains the Unet architecture
<br />

<b>tfwrapper</b><br />
This folder contains various scripts, e.g. loss functions, certain layer definitions, normalization etc.
<br />

<b>data_processing/batch_provider.py</b><br />
Batch provider to get pseudo-random sampling of batches while trying to avoid repetition as much as possible
<br />

<b>data_processing/heart_data.py</b><br />
Provides the heart data with the given data split, interacts with the model itself and the batch provider.
<br />

<b>Results</b><br />
Folder contains pickle files (and optionally images) with all results
Has a subfolder 'Plotting' where these results can be plotted.
<br />

