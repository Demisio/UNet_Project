# U-Net
The general architecture of the project is taken from:
https://git.ee.ethz.ch/baumgach/discriminative_learning_toolbox and modified where needed.
The modified Unet itself was written by Dr. Christine Tanner (CVL, ETH).

## Installation
1.  Clone or download the repository:
    <pre> git clone https://github.com/Demisio/UNet.git </pre>
    
2.  Create an environment for the packages to be installed. With conda, e.g: 
    <pre>conda create -n UDCT python=3.6</pre>

2.  Install requirements via: 
    <pre> pip install -r requirements.txt </pre>
    
4.  Required CUDA / CUDNN versions:
    <pre>Python 3.6, TF 1.4: CUDA 8.0 / CUDNN 6.0</pre>
    
## Creating a Dataset, Data Splits and Log Folders
1.  Images are given in ./Data/Heart/3D. Creating a hdf5 dataset is achieved by navigating into the folder dataprocessing
    and executing the file heart_augment_loader.py
    
    The parameters 'syn_data_path' and 'raw_data_path' data path have to be modified to correspond
    to the desired ground truth and raw data respectively. E.g:
    
    <pre>cd data_processing
    raw_data_path = './../Data/Heart/3D/Raw/'
    syn_data_path = './../Data/Heart/3D/Segmented_og_labels/'</pre>
    
    Also, choose a desired filename for the h5 file, e.g:
    
    <pre>filename = 'aug_heart_data.h5'</pre>
    
2.  Datasplits are already provided but can be generated by navigating into the folder ./train_test_split and executing
    the file create_splits.py
    <pre>cd train_test_split
    python create_splits.py</pre>
    
3.  A dictionary, called "logs" has to be created in the main Project folder (if not present). The dictionary should
    contain subfolders with the name of the experiment and 5 subsubfolders called fold1 until fold5.
    For the current project e.g.:
    <pre>./logs/Heart_full/fold4</pre>
    
4.  For creating a dataset with limited data, run heart_augment_loader.py once more but change parameters
    in the file as follows:
    <pre>raw_data_path = './../Data/Heart/3D/Raw_lim_data/'
    syn_data_path = './../Data/Heart/3D/Segmented_og_labels_lim_data/'</pre>
    
    Also, choose a different h5 filename, e.g.:
    
    <pre>filename = 'aug_heart_data_limited.h5'</pre>
    
## Changing Model configurations and Training the Model

1.  All relevant model parameters are defined in the file:
    <pre>./segmenter/experiments/heart_config.py</pre>
2.  Training a Model with different amounts of data requires changing the variable 'mode' as follows:
    <pre>mode = 'full'    #all standard augmented data
    mode = 'limited' #limited, standard augmented data
    mode = 'GAN'     #GAN augmented data</pre>
    
2.  Execute Training via:
    <pre>python segmenter_train.py</pre>
    
## Testing the Model
1.  After training (or when using the pretrained models), change the variable 'set' in heart_config.py depending on your needs:

    <pre>set = 'test' #use test set at test time
    set = 'train' #use training set with pre-trained model
    set = 'validation' #use validation set with pre-trained model</pre>
    
2.  Ensure that you have a dictionary in the folder 'Results' to save the relevant metrics, e.g.:
    <pre>./Results/Heart
    ./Results/Heart_limited</pre>
    
3.  Run the test as follows:
    <pre>python segmenter_test.py</pre>
    
4.  navigate into the folder 'Plotting':
    <pre>cd Results/Plotting</pre>
    
    and execute:
    <pre>python metric_plots.py</pre>
    
    make sure to set the variable inside the file 'evaldir' to the folder containing the relevant pickle file, e.g.:
    <pre>evaldir = './../Heart'
    evaldir = './../Heart_limited'</pre>
    
    and set the variable 'mode' inside the file to your needs (plots metrics for the desired set):
    <pre>mode = 'test'
    mode = 'train'
    mode = 'validation'</pre>
    